import os
import streamlit as st
import chromadb
import langchain

# [ë³€ê²½] ìµœì‹  íŒ¨í‚¤ì§€ ì„í¬íŠ¸ ê²½ë¡œ ì ìš©
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_community.chat_message_histories import StreamlitChatMessageHistory

# [ë³€ê²½] Ollama ê´€ë ¨ ì„í¬íŠ¸
from langchain_ollama import OllamaEmbeddings, ChatOllama

from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from langchain_core.runnables.history import RunnableWithMessageHistory

# LangChain ë²„ì „ì— ë”°ë¼ import ê²½ë¡œê°€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆì–´ í˜¸í™˜ ì²˜ë¦¬
try:
    from langchain.chains import create_history_aware_retriever
except ImportError:
    try:
        from langchain.chains.history_aware_retriever import create_history_aware_retriever
    except ImportError:
        create_history_aware_retriever = None

# ChromaDB Tenant ì˜¤ë¥˜ ë°©ì§€ (Streamlit ë¦¬ë¡œë“œ ì‹œ í•„ìˆ˜)
chromadb.api.client.SharedSystemClient.clear_system_cache()

# [ì„¤ì •] ì„ë² ë”© ëª¨ë¸ê³¼ ë²¡í„° DB ì €ì¥ ê²½ë¡œ ì„¤ì •
# ì£¼ì˜: OpenAIì™€ Ollama ì„ë² ë”©ì€ í˜¸í™˜ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ ê²½ë¡œë¥¼ ë¶„ë¦¬í–ˆìŠµë‹ˆë‹¤.
#CHROMA_PATH = "./chroma_db_ollama"
#EMBEDDING_MODEL = "nomic-embed-text" # Ollamaìš© ê³ ì„±ëŠ¥ ì„ë² ë”© ëª¨ë¸

EMBEDDING_MODEL = "qwen2.5-embedding" # Ollamaìš© QWEN ì„ë² ë”© ëª¨ë¸
CHROMA_PATH = "./chroma_db_ollama_{EMBEDDING_MODEL}" 

# cache_resourceë¡œ í•œë²ˆ ì‹¤í–‰í•œ ê²°ê³¼ ìºì‹±í•´ë‘ê¸°
@st.cache_resource
def load_and_split_pdf(file_path):
    # íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    if not os.path.exists(file_path):
        st.error(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        return []
    loader = PyPDFLoader(file_path)
    return loader.load_and_split()

# í…ìŠ¤íŠ¸ ì²­í¬ë“¤ì„ Chroma ì•ˆì— ì„ë² ë”© ë²¡í„°ë¡œ ì €ì¥
@st.cache_resource
def create_vector_store(_docs):
    if not _docs:
        return None
        
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    split_docs = text_splitter.split_documents(_docs)
    
    # [ë³€ê²½] OllamaEmbeddings ì‚¬ìš©
    vectorstore = Chroma.from_documents(
        split_docs, 
        OllamaEmbeddings(model=EMBEDDING_MODEL),
        persist_directory=CHROMA_PATH
    )
    return vectorstore

# ë§Œì•½ ê¸°ì¡´ì— ì €ì¥í•´ë‘” ChromaDBê°€ ìˆëŠ” ê²½ìš°, ì´ë¥¼ ë¡œë“œ
@st.cache_resource
def get_vectorstore(_docs):
    if os.path.exists(CHROMA_PATH) and os.listdir(CHROMA_PATH):
        return Chroma(
            persist_directory=CHROMA_PATH,
            embedding_function=OllamaEmbeddings(model=EMBEDDING_MODEL)
        )
    else:
        return create_vector_store(_docs)
    
# PDF ë¬¸ì„œ ë¡œë“œ-ë²¡í„° DB ì €ì¥-ê²€ìƒ‰ê¸°-íˆìŠ¤í† ë¦¬ ëª¨ë‘ í•©ì¹œ Chain êµ¬ì¶•
@st.cache_resource
def initialize_components(selected_model):
    # [ì£¼ì˜] ì‹¤ì œ íŒŒì¼ ê²½ë¡œì— ë§ê²Œ ìˆ˜ì •í•´ì£¼ì„¸ìš”.
    file_path = r"./data/ëŒ€í•œë¯¼êµ­í—Œë²•(í—Œë²•)(ì œ00010í˜¸)(19880225).pdf"
    
    pages = load_and_split_pdf(file_path)
    if not pages:
        return None
        
    vectorstore = get_vectorstore(pages)
    retriever = vectorstore.as_retriever()

    # ì±„íŒ… íˆìŠ¤í† ë¦¬ ìš”ì•½ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    contextualize_q_system_prompt = """Given a chat history and the latest user question \
    which might reference context in the chat history, formulate a standalone question \
    which can be understood without the chat history. Do NOT answer the question, \
    just reformulate it if needed and otherwise return it as is."""
    
    contextualize_q_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder("history"),
            ("human", "{input}"),
        ]
    )

    # ì§ˆë¬¸-ë‹µë³€ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    qa_system_prompt = """You are an assistant for question-answering tasks. \
    Use the following pieces of retrieved context to answer the question. \
    If you don't know the answer, just say that you don't know. \
    Keep the answer perfect. please use imogi with the answer.
    ëŒ€ë‹µì€ í•œêµ­ì–´ë¡œ í•˜ê³ , ì¡´ëŒ“ë§ì„ ì¨ì¤˜.\

    {context}"""
    
    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", qa_system_prompt),
            MessagesPlaceholder("history"),
            ("human", "{input}"),
        ]
    )

    # [ë³€ê²½] ChatOllama ì‚¬ìš©
    llm = ChatOllama(model=selected_model)
    
    # íˆìŠ¤í† ë¦¬ ê¸°ë°˜ ì§ˆì˜ ì¬êµ¬ì„± Retriever (ë²„ì „ì— ì—†ìœ¼ë©´ ì¼ë°˜ retriever ì‚¬ìš©)
    # âš ï¸ ì¼ë°˜ retrieverëŠ” "ë¬¸ìì—´ ì§ˆë¬¸"ë§Œ ë°›ì•„ì•¼ í•©ë‹ˆë‹¤. (dictê°€ ë“¤ì–´ê°€ë©´ EmbedRequest ValidationError ë°œìƒ)
    if create_history_aware_retriever is not None:
        history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)
    else:
        # RunnableWithMessageHistoryê°€ ë„˜ê¸°ëŠ” ì…ë ¥ì€ {"input": ì§ˆë¬¸, "history": ...} í˜•íƒœì´ë¯€ë¡œ
        # retrieverì—ëŠ” ì§ˆë¬¸ ë¬¸ìì—´ë§Œ ì „ë‹¬í•˜ë„ë¡ ë³€í™˜í•©ë‹ˆë‹¤.
        history_aware_retriever = RunnableLambda(lambda x: x["input"]) | retriever

    def _format_docs(docs):
        return "\n\n".join(getattr(d, "page_content", str(d)) for d in (docs or []))

    # LangChain ë²„ì „ì— ë”°ë¼ create_stuff_documents_chain APIê°€ ì—†ì„ ìˆ˜ ìˆì–´
    # ë™ì¼í•œ ë™ì‘(ë¬¸ì„œë“¤ì„ í•œ ë©ì–´ë¦¬ë¡œ 'stuff'í•˜ì—¬ í”„ë¡¬í”„íŠ¸ì— ë„£ê¸°)ì„ Runnableë¡œ ì§ì ‘ êµ¬ì„±
    rag_chain = (
        RunnablePassthrough
        .assign(context_docs=history_aware_retriever)
        .assign(context=RunnableLambda(lambda x: _format_docs(x["context_docs"])))
        .assign(
            answer=(
                qa_prompt
                | llm
                | RunnableLambda(lambda m: getattr(m, "content", str(m)))
            )
        )
        | RunnableLambda(lambda x: {"answer": x["answer"], "context": x["context_docs"]})
    )

    return rag_chain

# Streamlit UI
st.header("Taewan's RAG ì±—ë´‡ (Ollama Ver) ğŸ’¬ ğŸ“š")

# [ë³€ê²½] Ollama ëª¨ë¸ ì„ íƒì§€ë¡œ ë³€ê²½
option = st.selectbox("Select Ollama Model", ("llama3.2", "mistral", "gemma2"))

# ì²´ì¸ ì´ˆê¸°í™”
rag_chain = initialize_components(option)

chat_history = StreamlitChatMessageHistory(key="chat_messages")

if rag_chain:
    conversational_rag_chain = RunnableWithMessageHistory(
        rag_chain,
        lambda session_id: chat_history,
        input_messages_key="input",
        history_messages_key="history",
        output_messages_key="answer",
    )

    if "messages" not in st.session_state:
        st.session_state["messages"] = [{"role": "assistant", 
                                         "content": "í—Œë²•ì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!"}]

    for msg in chat_history.messages:
        st.chat_message(msg.type).write(msg.content)

    if prompt_message := st.chat_input("Your question"):
        st.chat_message("human").write(prompt_message)
        with st.chat_message("ai"):
            with st.spinner("Thinking..."):
                config = {"configurable": {"session_id": "any"}}
                response = conversational_rag_chain.invoke(
                    {"input": prompt_message},
                    config)
                
                answer = response['answer']
                st.write(answer)
                with st.expander("ì°¸ê³  ë¬¸ì„œ í™•ì¸"):
                    for doc in response['context']:
                        st.markdown(doc.metadata.get('source', 'Unknown'), help=doc.page_content)
else:
    st.error("PDF íŒŒì¼ì„ ë¡œë“œí•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")